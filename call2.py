import streamlit as st
import openai
import tiktoken
import re

encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

# Streamlit config
st.set_page_config(
    page_title="Call Analysis",
    layout='wide',
    page_icon="musical_note",
    menu_items={
         'About': 'Call Kevin MAMERI',
     }
)

# Set page title, header and links to docs
st.header("ğŸ—£ Call analysis POC using Python, Streamlit and OpenAIâœ¨")
st.caption(f"App developed by IntescIA ;)")

# Let the user input the OPENAI_API_KEY
openai_api_key = st.text_input("Please enter your OPENAI_API_KEY", type="password")

if openai_api_key:
    openai.api_key = openai_api_key
else:
    st.warning("Please enter your OPENAI_API_KEY to proceed.")
    st.stop()

upload_path = "upload_path/"

st.markdown("""---""")
st.subheader("Upload The Audio File or Text File Below")
uploaded_file = st.file_uploader("Choose a file", accept_multiple_files=False, type=["wav", "mp3", "txt"], label_visibility='hidden')

if uploaded_file is not None:
    file_name = uploaded_file.name
    file_path = f"./{file_name}"

    if file_name.endswith(".txt"):
        with st.spinner(f"Processing Text File ... ğŸ’«"):
            file_contents = uploaded_file.read().decode("utf-8")
            user_input = file_contents
    else:
        with st.spinner(f"Processing Audio ... ğŸ’«"):
            audio_bytes = uploaded_file.read()
            with open(file_path, "wb") as f:
                f.write(audio_bytes)

            with open(file_path, "rb") as audio_file:
                transcribe = openai.Audio.transcribe(audio_file)
                user_input = transcribe.text

        st.title("Generated Original Text ğŸ”Š")
        st.write(user_input)

    # Retirer les uuid du texte
    uuid_pattern = r'\b[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\b\s'
    user_input = re.sub(uuid_pattern, "", user_input)

    # RÃ©duire la longueur du texte si nÃ©cessaire
    max_tokens = 16_000  # DÃ©finir une limite pour Ã©viter de dÃ©passer la limite de tokens du modÃ¨le
    encoded_user_input = encoding.encode(user_input)
    if len(encoded_user_input) > max_tokens:
        user_input = encoding.decode(encoded_user_input[:max_tokens])
        st.warning("Le texte dÃ©passe la limite maximale de tokens du modÃ¨le. Il a Ã©tÃ© tronquÃ© Ã  16 000 tokens.")

    with st.spinner(f"Processing Text ... ğŸ’«"):
        model = "gpt-3.5-turbo-16k"
        system_input = "Tu es le directeur gÃ©nÃ©ral d'un grand Ã©diteur SAAS, tu dois analyser la dÃ©mo de notre commercial avec un prospect qui prÃ©sente notre applicatif, ta rÃ©ponse dont contenir 3 parties : 1_Les points positifs de cet Ã©change. 2_Les points Ã  amÃ©liorer dans sa prÃ©sentation. 3_ Les fonctionnalitÃ©s qu'on pourrait dÃ©velopper."
        res = openai.ChatCompletion.create(
          model=model,
          messages=[
                {"role": "system", "content": system_input},
                {"role": "user", "content": user_input},
            ]
        )
        st.title("Generated analysis ğŸ”Š")
        st.write(res.choices[0].message.content)
